{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pickle\n",
    "import os\n",
    "import types\n",
    "import random\n",
    "import uuid\n",
    "import math\n",
    "from copy import deepcopy as copy\n",
    "import logging\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.envs.classic_control import rendering\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.misc import logsumexp\n",
    "from baselines import deepq\n",
    "import baselines.common.tf_util as U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rc('savefig', dpi=300)\n",
    "mpl.rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "assert len(logger.handlers) == 1\n",
    "handler = logger.handlers[0]\n",
    "handler.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.join('data', '5.0-lander-ime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create envs, pilot policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "throttle_mag = 0.75\n",
    "def disc_to_cont(action):\n",
    "  if type(action) == np.ndarray:\n",
    "    return action\n",
    "  # main engine\n",
    "  if action < 3:\n",
    "    m = -throttle_mag\n",
    "  elif action < 6:\n",
    "    m = throttle_mag\n",
    "  else:\n",
    "    raise ValueError\n",
    "  # steering\n",
    "  if action % 3 == 0:\n",
    "    s = -throttle_mag\n",
    "  elif action % 3 == 1:\n",
    "    s = 0\n",
    "  else:\n",
    "    s = throttle_mag\n",
    "  return np.array([m, s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_act_dim = 6\n",
    "n_obs_dim = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_ep_len = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slow_fps = 60\n",
    "fast_fps = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fan_fpses = np.arange(fast_fps - (slow_fps - fast_fps), slow_fps + 2, 2)\n",
    "\n",
    "fan_confs = [{'fps': fps} for fps in fan_fpses]\n",
    "n_ims = len(fan_confs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aristotle_conf_idxes = [i for i, conf in enumerate(fan_confs) if conf['fps'] == 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_ims, fan_fpses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "...or random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf_bounds = {\n",
    "  'fps': (20, 60),\n",
    "  'mep': (6.5, 19.5),\n",
    "  'sep': (0.3, 0.9),\n",
    "  'sea': (6, 18),\n",
    "  'seh': (7, 21),\n",
    "  'scale': (15, 45),\n",
    "  'leg_down': (9, 27)\n",
    "}\n",
    "n_ims = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fan_confs = [{k: (v[0] + np.random.random() * (v[1] - v[0])) for k, v in conf_bounds.items()} for _ in range(n_ims)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_goals = np.arange(1, 10, 1).astype(int)\n",
    "n_train_tasks = train_goals.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_lander_env(fps=slow_fps, goal=None):\n",
    "  env = gym.make('LunarLanderContinuous-v2')\n",
    "  env.unwrapped.goal = goal\n",
    "  env.action_space = spaces.Discrete(n_act_dim)\n",
    "  env.unwrapped._step_orig = env.unwrapped._step\n",
    "  def _step(self, action):\n",
    "    obs, r, done, info = self._step_orig(disc_to_cont(action))\n",
    "    return obs, r, done, info\n",
    "  env.unwrapped._step = types.MethodType(_step, env.unwrapped)\n",
    "  env.unwrapped.fps = fps\n",
    "  return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_newton_envs = [make_lander_env(fps=fast_fps, goal=goal) for goal in train_goals]\n",
    "train_aristotle_envs = [make_lander_env(fps=slow_fps, goal=goal) for goal in train_goals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_ep(policy, env, max_ep_len=max_ep_len, render=False, task_idx=None):\n",
    "  obs = env.reset()\n",
    "  done = False\n",
    "  totalr = 0.\n",
    "  prev_obs = obs\n",
    "  rollout = []\n",
    "  for step_idx in range(max_ep_len+1):\n",
    "    if done:\n",
    "      break\n",
    "    action = policy(obs)\n",
    "    obs, r, done, info = env.step(action)\n",
    "    rollout.append((prev_obs, action, r, obs, float(done), task_idx))\n",
    "    prev_obs = obs\n",
    "    if render:\n",
    "      env.render()\n",
    "    totalr += r\n",
    "  return rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train agent with soft dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_aristotle_env = make_lander_env(fps=slow_fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_training_episodes = 800\n",
    "load_pretrained_pilot = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_q_func = lambda: deepq.models.mlp([64, 64])\n",
    "dqn_learn_kwargs = {\n",
    "  'lr': 1e-3,\n",
    "  'target_network_update_freq': 3000,\n",
    "  'print_freq': 100,\n",
    "  'max_timesteps': max_ep_len * (1 if load_pretrained_pilot else n_training_episodes)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'aristotle_dqn_pilot_scope.pkl'), 'rb') as f:\n",
    "  aristotle_dqn_pilot_scope = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aristotle_dqn_pilot_scope = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_aristotle_dqn_pilot_policy, _ = deepq.learn(\n",
    "  train_aristotle_env,\n",
    "  q_func=make_q_func(),\n",
    "  scope=aristotle_dqn_pilot_scope,\n",
    "  **dqn_learn_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'aristotle_dqn_pilot_scope.pkl'), 'wb') as f:\n",
    "  pickle.dump(aristotle_dqn_pilot_scope, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aristotle_dqn_pilot_path = os.path.join(data_dir, 'aristotle_dqn_pilot.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_tf_vars(sess, scope, path):\n",
    "  saver = tf.train.Saver([v for v in tf.global_variables() if v.name.startswith(scope + '/')])\n",
    "  saver.save(sess, save_path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_tf_vars(sess, scope, path):\n",
    "  saver = tf.train.Saver([v for v in tf.global_variables() if v.name.startswith(scope + '/')])\n",
    "  saver.restore(sess, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_tf_vars(U.get_session(), aristotle_dqn_pilot_scope, aristotle_dqn_pilot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_tf_vars(U.get_session(), aristotle_dqn_pilot_scope, aristotle_dqn_pilot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VIEWPORT_W = 600\n",
    "VIEWPORT_H = 400\n",
    "SCALE = 30.0\n",
    "W = VIEWPORT_W/SCALE\n",
    "H = VIEWPORT_H/SCALE\n",
    "CHUNKS = 11\n",
    "chunk_x = [W/(CHUNKS-1)*i for i in range(CHUNKS)]\n",
    "helipad_xs = [(chunk_x[goal-1]+chunk_x[goal+1])/2 for goal in train_goals]\n",
    "train_goal_obses = [(helipad_x - VIEWPORT_W/SCALE/2) / (VIEWPORT_W/SCALE/2) for helipad_x in helipad_xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temperature = 1\n",
    "def make_aristotle_pilot_policy(train_task_idx):\n",
    "  goal_obs = train_goal_obses[train_task_idx]\n",
    "  def aristotle_pilot_policy(obs):\n",
    "    my_obs = copy(obs)\n",
    "    my_obs[8] = goal_obs\n",
    "    with tf.variable_scope(aristotle_dqn_pilot_scope, reuse=None):\n",
    "      return raw_aristotle_dqn_pilot_policy._act(my_obs[None, :], temperature=temperature)[0]\n",
    "  return aristotle_pilot_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aristotle_pilot_policies = [make_aristotle_pilot_policy(train_task_idx) for train_task_idx in range(n_train_tasks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_aristotle_pilot_policy(train_task_idx):\n",
    "  return aristotle_pilot_policies[train_task_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sanity-check envs, agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_task_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_ep(aristotle_pilot_policies[train_task_idx], train_aristotle_envs[train_task_idx], render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_aristotle_envs[train_task_idx].close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_ep(aristotle_pilot_policies[train_task_idx], train_newton_envs[train_task_idx], render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_newton_envs[train_task_idx].close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit internal dynamics model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_train_rollouts_per_env = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "demo_rollouts = [[run_ep(aristotle_pilot_policies[train_task_idx], newton_env, render=False, task_idx=train_task_idx)\n",
    "                  for _ in range(n_train_rollouts_per_env)]\n",
    "                 for train_task_idx, newton_env in enumerate(train_newton_envs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'aristotle_pilot_policy_demo_rollouts.pkl'), 'wb') as f:\n",
    "  pickle.dump(demo_rollouts, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "demo_rollouts_path = os.path.join('data', '5.1-lander-newton', 'sid_pilot_policy_demo_rollouts.pkl')\n",
    "#demo_rollouts_path = os.path.join(data_dir, 'aristotle_pilot_policy_demo_rollouts.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(demo_rollouts_path, 'rb') as f:\n",
    "  demo_rollouts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mlp(\n",
    "    input_placeholder,\n",
    "    output_size,\n",
    "    scope,\n",
    "    n_layers=1,\n",
    "    size=256,\n",
    "    activation=tf.nn.relu,\n",
    "    output_activation=None,\n",
    "    reuse=False\n",
    "  ):\n",
    "  out = input_placeholder\n",
    "  with tf.variable_scope(scope, reuse=reuse):\n",
    "    for _ in range(n_layers):\n",
    "      out = tf.layers.dense(out, size, activation=activation)\n",
    "    out = tf.layers.dense(out, output_size, activation=output_activation)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_obs_feats = n_obs_dim\n",
    "featurize_obs = lambda s: s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def vectorize_rollouts(rollouts):\n",
    "  obs = [[] for _ in range(n_train_tasks)]\n",
    "  actions = [[] for _ in range(n_train_tasks)]\n",
    "  for task_idx, task_rollouts in enumerate(rollouts):\n",
    "    for task_rollout in task_rollouts:\n",
    "      more_obs, more_actions = list(zip(*task_rollout))[:2]\n",
    "      obs[task_idx].extend([featurize_obs(s) for s in more_obs])\n",
    "      actions[task_idx].extend(more_actions)\n",
    "  l = min(len(x) for x in obs)\n",
    "  idxes = [random.sample(list(range(len(x))), l) for x in obs]\n",
    "  f = lambda x: np.array(x[1])[idxes[x[0]]]\n",
    "  obs = np.array(list(map(f, enumerate(obs))))\n",
    "  actions = np.array(list(map(f, enumerate(actions))))\n",
    "  return obs, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "demo_obs = None\n",
    "demo_actions = None\n",
    "demo_next_obs = None\n",
    "demo_task_idxes = None\n",
    "train_demo_example_idxes = None\n",
    "val_demo_batch = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_demo_rollouts(demo_rollouts):\n",
    "  global demo_obs\n",
    "  global demo_actions\n",
    "  global demo_next_obs\n",
    "  global demo_task_idxes\n",
    "  global train_demo_example_idxes\n",
    "  global val_demo_batch\n",
    "\n",
    "  vectorized_demo_rollouts = vectorize_rollouts(demo_rollouts)\n",
    "  \n",
    "  demo_obs, demo_actions = vectorized_demo_rollouts\n",
    "  demo_example_idxes = list(range(demo_obs.shape[1]))\n",
    "  \n",
    "  random.shuffle(demo_example_idxes)\n",
    "  n_train_demo_examples = int(0.9 * len(demo_example_idxes))\n",
    "  train_demo_example_idxes = demo_example_idxes[:n_train_demo_examples]\n",
    "  val_demo_example_idxes = demo_example_idxes[n_train_demo_examples:]\n",
    "  val_demo_batch = demo_obs[:, val_demo_example_idxes], demo_actions[:, val_demo_example_idxes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "process_demo_rollouts(demo_rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_batch(size):\n",
    "  idxes = random.sample(train_demo_example_idxes, size)\n",
    "  demo_batch = demo_obs[:, idxes], demo_actions[:, idxes]\n",
    "  return demo_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "iterations = 100000\n",
    "learning_rate = 1e-3\n",
    "batch_size = 512 // n_train_tasks\n",
    "sq_td_err_penalty = 1e-3\n",
    "\n",
    "q_n_layers = 1\n",
    "q_layer_size = 32\n",
    "q_activation = tf.nn.relu\n",
    "q_output_activation = None\n",
    "\n",
    "constraint_sampling_freq = 100000\n",
    "constraint_batch_size = batch_size\n",
    "n_constraint_rollouts_per_env = 100\n",
    "\n",
    "val_update_freq = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "im_scope = str(uuid.uuid4())\n",
    "q_scope = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "demo_obs_t_ph = tf.placeholder(tf.float32, [n_train_tasks, None, n_obs_feats])\n",
    "demo_act_t_ph = tf.placeholder(tf.int32, [n_train_tasks, None])\n",
    "demo_batch_size_ph = tf.placeholder(tf.int32)\n",
    "\n",
    "constraint_obs_t_ph = tf.placeholder(tf.float32, [n_train_tasks, None, n_obs_feats])\n",
    "constraint_act_t_ph = tf.placeholder(tf.int32, [n_train_tasks, None])\n",
    "constraint_obs_tp1_ph = tf.placeholder(tf.float32, [n_train_tasks, None, n_ims, n_obs_feats])\n",
    "constraint_rew_t_ph = tf.placeholder(tf.float32, [n_train_tasks, None, n_ims])\n",
    "constraint_batch_size_ph = tf.placeholder(tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "demo_batch_idxes = tf.reshape(\n",
    "  tf.range(0, demo_batch_size_ph, 1), \n",
    "  [demo_batch_size_ph, 1])\n",
    "\n",
    "extract_task = lambda x, i: tf.squeeze(tf.gather(x, tf.convert_to_tensor(\n",
    "  [i], dtype=tf.int32)), axis=[0]) \n",
    "\n",
    "demo_q_t = tf.stack([tf.gather_nd(\n",
    "  build_mlp(\n",
    "    extract_task(demo_obs_t_ph, train_task_idx),\n",
    "    n_act_dim, q_scope+'-'+str(train_task_idx), \n",
    "    n_layers=q_n_layers, size=q_layer_size,\n",
    "    activation=q_activation, output_activation=q_output_activation\n",
    "  ), \n",
    "  tf.concat([\n",
    "    demo_batch_idxes, \n",
    "    tf.expand_dims(extract_task(demo_act_t_ph, train_task_idx), 1)], axis=1)\n",
    ") for train_task_idx in range(n_train_tasks)], axis=0)\n",
    "\n",
    "demo_v_t = tf.reduce_logsumexp(\n",
    "  tf.stack([build_mlp(\n",
    "    extract_task(demo_obs_t_ph, train_task_idx),\n",
    "    n_act_dim, q_scope+'-'+str(train_task_idx), \n",
    "    n_layers=q_n_layers, size=q_layer_size,\n",
    "    activation=q_activation, output_activation=q_output_activation,\n",
    "    reuse=True\n",
    "  ) for train_task_idx in range(n_train_tasks)], axis=0),\n",
    "  axis=2)\n",
    "\n",
    "act_log_likelihoods = demo_q_t - demo_v_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neg_avg_log_likelihood = -tf.reduce_mean(act_log_likelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_tp1 = tf.stack([tf.reshape(\n",
    "  build_mlp(\n",
    "    tf.reshape(\n",
    "      extract_task(constraint_obs_tp1_ph, train_task_idx),\n",
    "      [constraint_batch_size_ph*n_ims, n_obs_feats]), \n",
    "    n_act_dim, q_scope+'-'+str(train_task_idx),\n",
    "    n_layers=q_n_layers, size=q_layer_size,\n",
    "    activation=q_activation, output_activation=q_output_activation, \n",
    "    reuse=True\n",
    "  ), \n",
    "  [constraint_batch_size_ph, n_ims, n_act_dim]\n",
    ") for train_task_idx in range(n_train_tasks)], axis=0)\n",
    "v_tp1 = tf.reduce_logsumexp(q_tp1, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im_probs = tf.get_variable(im_scope, [n_ims], initializer=tf.random_normal_initializer)\n",
    "im_probs = tf.exp(im_probs) / tf.reduce_sum(tf.exp(im_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp_v_tp1 = tf.reduce_sum(im_probs * v_tp1, axis=2)\n",
    "exp_rew_t = tf.reduce_sum(im_probs * constraint_rew_t_ph, axis=2)\n",
    "target_t = exp_rew_t + gamma * exp_v_tp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "constraint_batch_idxes = tf.reshape(\n",
    "  tf.range(0, constraint_batch_size_ph, 1), \n",
    "  [constraint_batch_size_ph, 1])\n",
    "\n",
    "q_t = tf.stack([tf.gather_nd(\n",
    "  build_mlp(\n",
    "    extract_task(constraint_obs_t_ph, train_task_idx), \n",
    "    n_act_dim, q_scope+'-'+str(train_task_idx), \n",
    "    n_layers=q_n_layers, size=q_layer_size,\n",
    "    activation=q_activation, output_activation=q_output_activation, \n",
    "    reuse=True\n",
    "  ), \n",
    "  tf.concat([\n",
    "    constraint_batch_idxes, \n",
    "    tf.expand_dims(extract_task(constraint_act_t_ph, train_task_idx), 1)], axis=1)\n",
    ") for train_task_idx in range(n_train_tasks)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "td_err = q_t - target_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sq_td_err = tf.reduce_mean(td_err**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss = neg_avg_log_likelihood + sq_td_err_penalty * sq_td_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "update_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_int_dyn_nll():\n",
    "  im_probs_eval = sess.run(im_probs)\n",
    "  #int_dyn_nll = -np.log(1e-9+im_probs_eval[aristotle_conf_idxes]).sum()\n",
    "  int_dyn_nll = np.max(im_probs_eval)\n",
    "  return {'int_dyn_nll': int_dyn_nll}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_constraints(_):\n",
    "  constraint_rollouts = [[] for _ in range(n_train_tasks)]\n",
    "  \n",
    "  for train_task_idx in range(n_train_tasks):\n",
    "    rollouts = [[] for _ in range(n_constraint_rollouts_per_env)]\n",
    "    envs = [make_lander_env(\n",
    "      fps=fast_fps, goal=train_goals[train_task_idx]) for _ in range(\n",
    "      n_constraint_rollouts_per_env)]\n",
    "    obses = np.array([env.reset() for env in envs])\n",
    "    dones = [False for _ in envs]\n",
    "    prev_obses = obses\n",
    "    for step_idx in range(max_ep_len+1):\n",
    "      not_done_idxes = [i for i, done in enumerate(dones) if not done]\n",
    "      batch_size = len(not_done_idxes)\n",
    "      if batch_size == 0:\n",
    "        break\n",
    "      actions = np.random.choice(n_act_dim, batch_size)\n",
    "      for i, env_idx in enumerate(not_done_idxes):\n",
    "        env = envs[env_idx]\n",
    "        action = actions[i]\n",
    "        env.unwrapped.fan = True\n",
    "        env.unwrapped.fan_confs = fan_confs\n",
    "        obs, r, done, info = env.step(action)\n",
    "        obses[env_idx] = obs\n",
    "        dones[env_idx] = done\n",
    "        rollouts[env_idx].append((\n",
    "          prev_obses[env_idx], action, info['rews'], info['obses']))\n",
    "      prev_obses = copy(obses)\n",
    "    constraint_rollouts[train_task_idx].extend([r for r in rollouts if r != []])\n",
    "\n",
    "  size = min(sum(len(r) for r in rollouts) for rollouts in constraint_rollouts)\n",
    "  \n",
    "  global train_constraint_example_idxes\n",
    "  global val_constraint_batch\n",
    "  global constraint_obs_t\n",
    "  global constraint_act_t\n",
    "  global constraint_obs_tp1\n",
    "  global constraint_rew_t\n",
    "    \n",
    "  constraint_obs_t = np.zeros((n_train_tasks, size, n_obs_feats))\n",
    "  constraint_act_t = np.zeros((n_train_tasks, size))\n",
    "  constraint_obs_tp1 = np.zeros((n_train_tasks, size, n_ims, n_obs_feats))\n",
    "  constraint_rew_t = np.zeros((n_train_tasks, size, n_ims))\n",
    "  \n",
    "  for train_task_idx in range(n_train_tasks):\n",
    "    unfeat_obses, actions, rews, next_obses = list(zip(*sum(\n",
    "      constraint_rollouts[train_task_idx], [])))\n",
    "    obses = [featurize_obs(s) for s in unfeat_obses]\n",
    "    next_obses = [[featurize_obs(s) for s in fan_s] for fan_s in next_obses]\n",
    "    idxes = random.sample(list(range(len(obses))), size)\n",
    "    constraint_obs_t[train_task_idx, :, :] = np.array(obses)[idxes, :]\n",
    "    constraint_act_t[train_task_idx, :] = np.array(actions)[idxes]\n",
    "    constraint_obs_tp1[train_task_idx, :, :, :] = np.array(next_obses)[idxes, :, :]\n",
    "    constraint_rew_t[train_task_idx, :, :] = np.array(rews)[idxes, :]\n",
    "  \n",
    "  constraint_example_idxes = list(range(size))\n",
    "  random.shuffle(constraint_example_idxes)\n",
    "  n_train_constraint_examples = int(0.9 * size)\n",
    "  \n",
    "  train_constraint_example_idxes = constraint_example_idxes[:n_train_constraint_examples]\n",
    "  val_constraint_example_idxes = constraint_example_idxes[n_train_constraint_examples:]\n",
    "  val_constraint_batch = constraint_obs_t[:, val_constraint_example_idxes], constraint_act_t[:, val_constraint_example_idxes], constraint_rew_t[:, val_constraint_example_idxes], constraint_obs_tp1[:, val_constraint_example_idxes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_constraint_batch(size):\n",
    "  global n_iters_since_prev_constraint_sample\n",
    "  if n_iters_since_prev_constraint_sample % constraint_sampling_freq == 0:\n",
    "    sample_constraints(size)\n",
    "    n_iters_since_prev_constraint_sample = 0\n",
    "  n_iters_since_prev_constraint_sample += 1\n",
    "\n",
    "  idxes = random.sample(train_constraint_example_idxes, size)\n",
    "  constraint_batch = constraint_obs_t[:, idxes], constraint_act_t[:, idxes], constraint_rew_t[:, idxes], constraint_obs_tp1[:, idxes]\n",
    "  return constraint_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_constraint_example_idxes = None\n",
    "val_constraint_batch = None\n",
    "constraint_obs_t = None\n",
    "constraint_act_t = None\n",
    "constraint_obs_tp1 = None\n",
    "constraint_rew_t = None\n",
    "n_iters_since_prev_constraint_sample = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'constraint_samples.pkl'), 'wb') as f:\n",
    "  pickle.dump((\n",
    "    train_constraint_example_idxes, \n",
    "    val_constraint_batch,\n",
    "    constraint_obs_t,\n",
    "    constraint_act_t,\n",
    "    constraint_obs_tp1,\n",
    "    constraint_rew_t,\n",
    "    n_iters_since_prev_constraint_sample), f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'constraint_samples.pkl'), 'rb') as f:\n",
    "  (\n",
    "    train_constraint_example_idxes, \n",
    "    val_constraint_batch,\n",
    "    constraint_obs_t,\n",
    "    constraint_act_t,\n",
    "    constraint_obs_tp1,\n",
    "    constraint_rew_t,\n",
    "    n_iters_since_prev_constraint_sample) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_iters = iterations * demo_obs.shape[1] // batch_size\n",
    "train_logs = {\n",
    "  'loss_evals': [],\n",
    "  'nll_evals': [],\n",
    "  'ste_evals': [],\n",
    "  'val_loss_evals': [],\n",
    "  'val_nll_evals': [],\n",
    "  'val_ste_evals': [],\n",
    "  'int_dyn_nll_evals': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_batch_loss(demo_batch, constraint_batch, step=False, t=None):\n",
    "  demo_batch_obs_t, demo_batch_act_t = demo_batch\n",
    "  constraint_batch_obs_t, constraint_batch_act_t, constraint_batch_rew_t, constraint_batch_obs_tp1 = constraint_batch\n",
    "  \n",
    "  feed_dict = {\n",
    "    demo_obs_t_ph: demo_batch_obs_t,\n",
    "    demo_act_t_ph: demo_batch_act_t,\n",
    "    demo_batch_size_ph: demo_batch_obs_t.shape[1],\n",
    "    constraint_obs_t_ph: constraint_batch_obs_t,\n",
    "    constraint_act_t_ph: constraint_batch_act_t,\n",
    "    constraint_obs_tp1_ph: constraint_batch_obs_tp1,\n",
    "    constraint_rew_t_ph: constraint_batch_rew_t,\n",
    "    constraint_batch_size_ph: constraint_batch_obs_t.shape[1],\n",
    "  }\n",
    "  \n",
    "  [loss_eval, neg_avg_log_likelihood_eval, sq_td_err_eval] = sess.run(\n",
    "    [loss, neg_avg_log_likelihood, sq_td_err], feed_dict=feed_dict)\n",
    "  \n",
    "  if step:\n",
    "    sess.run(update_op, feed_dict=feed_dict)\n",
    "  \n",
    "  d = {\n",
    "    'loss': loss_eval,\n",
    "    'nll': neg_avg_log_likelihood_eval,\n",
    "    'ste': sq_td_err_eval\n",
    "  }\n",
    "  if not step:\n",
    "    d.update(compute_int_dyn_nll())\n",
    "  return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_log = None\n",
    "while len(train_logs['loss_evals']) < n_iters:\n",
    "  demo_batch = sample_batch(batch_size)\n",
    "  constraint_batch = sample_constraint_batch(constraint_batch_size)\n",
    "  \n",
    "  t = len(train_logs['loss_evals'])\n",
    "  train_log = compute_batch_loss(demo_batch, constraint_batch, step=True, t=t)\n",
    "  if val_log is None or len(train_logs['loss_evals']) % val_update_freq == 0:\n",
    "    val_log = compute_batch_loss(val_demo_batch, val_constraint_batch, step=False, t=t)\n",
    "  \n",
    "  print('%d %d %f %f %f %f %f %f %f' % (\n",
    "    t, n_iters, train_log['loss'],\n",
    "    train_log['nll'], train_log['ste'], val_log['loss'],\n",
    "    val_log['nll'], val_log['ste'], val_log['int_dyn_nll'])\n",
    "  )\n",
    "  \n",
    "  for k, v in train_log.items():\n",
    "    train_logs['%s_evals' % k].append(v)\n",
    "  for k, v in val_log.items():\n",
    "    train_logs['%s%s_evals' % ('val_' if k in ['loss', 'nll', 'ste'] else '', k)].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k in ['val_nll_evals', 'val_ste_evals']:\n",
    "  plt.xlabel('Iterations')\n",
    "  plt.ylabel(k.split('_')[1])\n",
    "  plt.plot(train_logs[k])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Negative Log-Likelihood')\n",
    "plt.plot(train_logs['int_dyn_nll_evals'], color='orange')\n",
    "plt.axhline(y=-np.log(1/n_ims), linestyle='--', color='gray', label='Uniform')\n",
    "plt.ylim([-0.05, None])\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im_probs_eval = sess.run(im_probs)\n",
    "newton_fps = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpses = [conf['fps'] for conf in fan_confs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mpl.rcParams.update({'font.size': 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Game Speed')\n",
    "plt.ylabel('Likelihood')\n",
    "plt.title('Lunar Lander User Study')\n",
    "speeds = 1/np.array(fan_fpses[::-1])\n",
    "width = [y-x for x, y in zip(speeds[:-1], speeds[1:])]\n",
    "width.append(width[-1])\n",
    "plt.bar(\n",
    "  speeds, im_probs_eval[::-1], linewidth=0, color='orange', \n",
    "  width=width,\n",
    "  label='Internal Dynamics')\n",
    "plt.axvline(x=1/newton_fps, linestyle='--', label='Real Dynamics', color='gray')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(os.path.join(data_dir, 'human-speed-distrn.pdf'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inverse real dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_unitialized_tf_vars():\n",
    "  uninitialized_vars = []\n",
    "  for var in tf.all_variables():\n",
    "    try:\n",
    "      sess.run(var)\n",
    "    except tf.errors.FailedPreconditionError:\n",
    "      uninitialized_vars.append(var)\n",
    "  tf.initialize_variables(uninitialized_vars).run(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NNInvDynamicsModel():\n",
    "  \n",
    "  def __init__(self,\n",
    "      n_layers,\n",
    "      size,\n",
    "      activation,\n",
    "      normalization,\n",
    "      batch_size,\n",
    "      iterations,\n",
    "      learning_rate,\n",
    "      sess,\n",
    "      invdyn_scope\n",
    "    ):\n",
    "    self.scope = invdyn_scope\n",
    "    with tf.variable_scope(self.scope, reuse=None):\n",
    "      self.obs_t_ph = tf.placeholder(tf.float32, [None, n_obs_dim])\n",
    "      self.obs_delta_t_ph = tf.placeholder(tf.float32, [None, n_obs_dim])\n",
    "      self.act_t_ph = tf.placeholder(tf.int32, [None])\n",
    "      obs_cat_delta_t = tf.concat([self.obs_t_ph, self.obs_delta_t_ph], axis=1)\n",
    "      self.act_logits = build_mlp(\n",
    "        obs_cat_delta_t, n_act_dim, invdyn_scope, n_layers=n_layers, size=size,\n",
    "        activation=activation\n",
    "      )\n",
    "      self.act_preds = tf.argmax(self.act_logits, axis=1)\n",
    "      self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=self.act_t_ph,\n",
    "        logits=self.act_logits,\n",
    "      ))\n",
    "\n",
    "      self.update_op = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "\n",
    "      init_unitialized_tf_vars()\n",
    "\n",
    "    self.sess = sess\n",
    "    self.iterations = iterations\n",
    "    self.batch_size = batch_size\n",
    "    self.normalization = normalization\n",
    "\n",
    "  def fit(self, data):\n",
    "    obs, actions, rewards, next_obs, dones = data\n",
    "    mean_obs, std_obs, mean_deltas, std_deltas = self.normalization\n",
    "    normed_obs = normalize(obs, mean_obs, std_obs)\n",
    "    deltas = next_obs - obs\n",
    "    normed_deltas = normalize(deltas, mean_deltas, std_deltas)\n",
    "\n",
    "    example_idxes = range(len(obs))\n",
    "    def sample_batch(size):\n",
    "      idxes = random.sample(example_idxes, size)\n",
    "      return normed_obs[idxes], actions[idxes], normed_deltas[idxes]\n",
    "\n",
    "    n_iters = self.iterations * len(obs) // self.batch_size\n",
    "    with tf.variable_scope(self.scope, reuse=None):\n",
    "      for i in range(n_iters):\n",
    "        batch_obs_t, batch_act_t, batch_obs_delta = sample_batch(self.batch_size)\n",
    "        feed_dict = {\n",
    "          self.obs_t_ph: batch_obs_t,\n",
    "          self.act_t_ph: batch_act_t,\n",
    "          self.obs_delta_t_ph: batch_obs_delta\n",
    "        }\n",
    "        [loss, _] = self.sess.run([self.loss, self.update_op], feed_dict=feed_dict)\n",
    "        print('%d %d %f' % (i, n_iters, loss))\n",
    "\n",
    "  def predict(self, states, next_states):\n",
    "    mean_obs, std_obs, mean_deltas, std_deltas = self.normalization\n",
    "    normed_states = normalize(states, mean_obs, std_obs)\n",
    "    normed_deltas = normalize(next_states - states, mean_deltas, std_deltas)\n",
    "    with tf.variable_scope(self.scope, reuse=None):\n",
    "      feed_dict = {\n",
    "        self.obs_t_ph: normed_states,\n",
    "        self.obs_delta_t_ph: normed_deltas\n",
    "      }\n",
    "      return self.sess.run(self.act_preds, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_rollouts(rollouts):\n",
    "  obs = []\n",
    "  actions = []\n",
    "  rewards = []\n",
    "  next_obs = []\n",
    "  dones = []\n",
    "  for task_rollouts in rollouts:\n",
    "    for rollout in task_rollouts:\n",
    "      more_obs, more_actions, more_rewards, more_next_obs, more_dones, _ = list(zip(*rollout))\n",
    "      obs.extend(more_obs)\n",
    "      actions.extend(more_actions)\n",
    "      rewards.extend(more_rewards)\n",
    "      next_obs.extend(more_next_obs)\n",
    "      dones.extend(more_dones)\n",
    "  return np.array(obs), np.array(actions), np.array(rewards), np.array(next_obs), np.array(dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorized_demo_rollouts = vectorize_rollouts(demo_rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_normalization(vectorized_rollouts):\n",
    "  obs, actions, rewards, next_obs, _ = vectorized_rollouts\n",
    "  mean_obs = np.mean(obs, axis=0)\n",
    "  std_obs = np.std(obs, axis=0)\n",
    "  deltas = next_obs - obs\n",
    "  mean_deltas = np.mean(deltas, axis=0)\n",
    "  std_deltas = np.std(deltas, axis=0)\n",
    "  return mean_obs, std_obs, mean_deltas, std_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(data, mean, std, eps=1e-9):\n",
    "  return (data - mean) / (std + eps)\n",
    "\n",
    "def unnormalize(data, mean, std, eps=1e-9):\n",
    "  return data * (std + eps) + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalization = compute_normalization(vectorized_demo_rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_layers = 2\n",
    "layer_size = 64\n",
    "activation = tf.nn.relu\n",
    "learning_rate = 1e-4\n",
    "batch_size = 64\n",
    "iterations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'invdyn_scope.pkl'), 'rb') as f:\n",
    "  invdyn_scope = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "invdyn_scope = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_invdyn_model = NNInvDynamicsModel(\n",
    "  n_layers=n_layers,\n",
    "  size=layer_size,\n",
    "  activation=activation,\n",
    "  normalization=normalization,\n",
    "  batch_size=batch_size,\n",
    "  iterations=iterations,\n",
    "  learning_rate=learning_rate,\n",
    "  sess=sess,\n",
    "  invdyn_scope=invdyn_scope\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_invdyn_model.fit(vectorized_demo_rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'invdyn_scope.pkl'), 'wb') as f:\n",
    "  pickle.dump(invdyn_scope, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "invdyn_path = os.path.join(data_dir, 'invdyn.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_tf_vars(sess, invdyn_scope, invdyn_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_tf_vars(sess, invdyn_scope, invdyn_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'invdyn_normalization.pkl'), 'wb') as f:\n",
    "  pickle.dump(normalization, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "internal2real dynamics transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assisted_conf = fan_confs[np.argmax(sess.run(im_probs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_assisted_env():\n",
    "  env = gym.make('LunarLanderContinuous-v2')\n",
    "  env.action_space = spaces.Discrete(n_act_dim)\n",
    "  env.unwrapped._step_orig = env.unwrapped._step\n",
    "  def _step(self, action):\n",
    "    if type(action) == np.int64 or len(action) == 1:\n",
    "      if type(action) == np.ndarray:\n",
    "        action = action[0]\n",
    "        \n",
    "      if self.curr_obs is not None:\n",
    "        intended_state = self.sim_step(disc_to_cont(action), **assisted_conf)[0]\n",
    "        intended_action = true_invdyn_model.predict(\n",
    "          np.array([self.curr_obs]), np.array([intended_state]))[0]\n",
    "      else:\n",
    "        intended_action = action\n",
    "        \n",
    "      obs, r, done, info = self._step_orig(disc_to_cont(intended_action))\n",
    "      return obs, r, done, info\n",
    "    else:\n",
    "      return self._step_orig(action)\n",
    "  env.unwrapped._step = types.MethodType(_step, env.unwrapped)\n",
    "  env.unwrapped.fps = fast_fps\n",
    "  \n",
    "  test_task_idx = np.random.choice(n_train_tasks)\n",
    "  test_aristotle_pilot_policy = make_aristotle_pilot_policy(test_task_idx)\n",
    "  env.unwrapped.goal = train_goals[test_task_idx]\n",
    "  \n",
    "  return test_aristotle_pilot_policy, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_env_without_dyn_transfer(using_slow_fps):\n",
    "  test_task_idx = np.random.choice(n_train_tasks)\n",
    "  test_aristotle_pilot_policy = make_aristotle_pilot_policy(test_task_idx)\n",
    "  unassisted_env = train_newton_envs[test_task_idx] if not using_slow_fps else train_aristotle_envs[test_task_idx]\n",
    "  return test_aristotle_pilot_policy, unassisted_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_unassisted_env = lambda: make_env_without_dyn_transfer(using_slow_fps=False)\n",
    "make_ideal_env = lambda: make_env_without_dyn_transfer(using_slow_fps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_eval_rollouts = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assisted_rollouts = [run_ep(*make_assisted_env(), render=False) for _ in range(n_eval_rollouts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'aristotle_pilot_policy_assisted_rollouts.pkl'), 'wb') as f:\n",
    "  pickle.dump(assisted_rollouts, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'aristotle_pilot_policy_assisted_rollouts.pkl'), 'rb') as f:\n",
    "  assisted_rollouts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unassisted_rollouts = [run_ep(*make_unassisted_env(), render=False) for _ in range(n_eval_rollouts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'aristotle_pilot_policy_unassisted_rollouts.pkl'), 'wb') as f:\n",
    "  pickle.dump(unassisted_rollouts, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'aristotle_pilot_policy_unassisted_rollouts.pkl'), 'rb') as f:\n",
    "  unassisted_rollouts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ideal_rollouts = [run_ep(*make_ideal_env(), render=False) for _ in range(n_eval_rollouts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'aristotle_pilot_policy_ideal_rollouts.pkl'), 'wb') as f:\n",
    "  pickle.dump(ideal_rollouts, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'aristotle_pilot_policy_ideal_rollouts.pkl'), 'rb') as f:\n",
    "  ideal_rollouts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unassisted_rew = [sum(x[2] for x in r) for r in unassisted_rollouts]\n",
    "ideal_rew = [sum(x[2] for x in r) for r in ideal_rollouts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assisted_rew = [sum(x[2] for x in r) for r in assisted_rollouts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean(unassisted_rew), np.mean(ideal_rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(assisted_rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "succ_rew_bonus = 100\n",
    "crash_rew_penalty = -100\n",
    "is_succ = lambda r: r[-1][2] > succ_rew_bonus / 2\n",
    "is_crash = lambda r: r[-1][2] < crash_rew_penalty / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unassisted_succ = [1 if is_succ(r) else 0 for r in unassisted_rollouts]\n",
    "ideal_succ = [1 if is_succ(r) else 0 for r in ideal_rollouts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assisted_succ = [1 if is_succ(r) else 0 for r in assisted_rollouts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(unassisted_succ), np.mean(ideal_succ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(assisted_succ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unassisted_crash = [1 if is_crash(r) else 0 for r in unassisted_rollouts]\n",
    "ideal_crash = [1 if is_crash(r) else 0 for r in ideal_rollouts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assisted_crash = [1 if is_crash(r) else 0 for r in assisted_rollouts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(unassisted_crash), np.mean(ideal_crash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(assisted_crash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_ep(*make_assisted_env(), render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
